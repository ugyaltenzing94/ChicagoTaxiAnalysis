{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tce3stUlHN0L"
   },
   "source": [
    "##### Copyright Deloitte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qFdPvlXBOdUN"
   },
   "source": [
    "# Chicago Taxi Geographic zones prediction (Evaluation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xHxb-dlhMIzW"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook is intended to showcase the steps taken to train a model for the Chicago Taxi Geographic zones prediction. Before we jump into the detailed steps used, let us get an eagle view of the entire process. Model Development has the following life cycle:\n",
    "\n",
    "1. Data Preprocessing\n",
    "2. Model Design and Training \n",
    "3. Model Evaluation and HyperParameter Tuning **[Here]**\n",
    "4. Model Deployment and Inference\n",
    "\n",
    "Our current hypothesis dictates the prediction of the High Demand Geographic zones based on several factors (more details provided in the technical document and white paper). This notebook details primarily with the Model Training aspect of the Modeling Life Cycle and showcases the necessary steps required to run a training job using AI Platform.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MUXex9ctTuDB"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4YsfgDMZW5g6"
   },
   "source": [
    "Set up your GCP project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "#### 1. Define your configuration Variables :\n",
    "\n",
    "This is the process of providing static configuration details such as the Project Name, Bucket Name etc along with some variable details such as the Number of Epochs etc. The static details can be changed to work with any other GCP project while the Variable details can be used to run HyperParameter Tuning jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "# Use project ID from console page\n",
    "!gcloud config set project us-gcp-ame-con-01e-npd-1\n",
    "\n",
    "#project name\n",
    "PROJECT_NAME = \"us-gcp-ame-con-01e-npd-1\"\n",
    "\n",
    "# Shows up in models pane of AI platform, helps you track your key project\n",
    "MODEL_NAME = \"Evaluation_One_Final\"\n",
    "\n",
    "# This defines your each experiment, try to keep it meaningful\n",
    "MODEL_VERSION = \"v3\"\n",
    "\n",
    "#region\n",
    "REGION = \"us-east4\"\n",
    "\n",
    "# JOB_NAME is auto-incremental. It will be appended by the latest timestamp when submitting the training job.\n",
    "JOB_NAME = MODEL_NAME + '_' + MODEL_VERSION\n",
    "\n",
    "#Google Storage Bucket\n",
    "BUCKET_NAME = \"us-gcp-ame-con-01e-npd-1-modelartifacts\"\n",
    "\n",
    "#sub folder name \n",
    "SUB_FOLDER_NAME = MODEL_NAME + \"/\" + MODEL_VERSION\n",
    "\n",
    "# Creates gs directory with this name - persists transient files during AI Platform jobs tun\n",
    "JOB_DIR = 'gs://' + BUCKET_NAME + '/' + SUB_FOLDER_NAME\n",
    "\n",
    "# Should be same as JOB_DIR with model_save as folder added\n",
    "MODEL_DIR = JOB_DIR + \"/model_save/\"\n",
    "\n",
    "#The number of times the data is shown to the Model. This is a hyper parameter and can be tuned\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "#The restriction placed on the dataset to retrive our train data. \n",
    "TRAIN_RESTRICTION = \"'trip_start_timestamp <= 1546300800'\"\n",
    "\n",
    "#The restriction placed on the dataset to retrieve our validation data\n",
    "EVAL_RESTRICTION = \"'trip_start_timestamp > 1546300800 and trip_start_timestamp<=1567276140'\"\n",
    "\n",
    "#Batch size for training\n",
    "TRAIN_BATCH_SIZE = 5120\n",
    "\n",
    "#Batch size for evaluation\n",
    "EVAL_BATCH_SIZE = 5120\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create a skeleton for the Trainer\n",
    "According to the AI Platform documentation the training code has to be modularised into a Trainer package with a defined layout. The following steps create that layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Delete the folder if it already exist or change the MODEL_VERSION to keep it unique\n",
    "import os, shutil\n",
    "\n",
    "PACKAGE_NAME = 'evaluation1'\n",
    "\n",
    "MODEL_FILE_PATH = '{}'.format(PACKAGE_NAME)\n",
    "\n",
    "if os.path.isdir(MODEL_FILE_PATH):\n",
    "    \n",
    "    shutil.rmtree(MODEL_FILE_PATH)\n",
    "    \n",
    "!mkdir $MODEL_FILE_PATH\n",
    "\n",
    "trainer_path = MODEL_FILE_PATH + '/trainer.py'\n",
    "\n",
    "init_path = MODEL_FILE_PATH + '/__init__.py'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Create the Setup File\n",
    "\n",
    "The Setup File is required to download any additional package that was used during the Modeling process. This is placed outside the Trainer model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setup filename: setup.py\n"
     ]
    }
   ],
   "source": [
    "SETUP_FILENAME =  \"setup.py\"\n",
    "\n",
    "print(\"setup filename: \" + SETUP_FILENAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $SETUP_FILENAME\n",
    "\n",
    "# Setup file to install necessary packages\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "# You can define all the packages you need for your job below along with the versions\n",
    "\n",
    "REQUIRED_PACKAGES = ['tensorflow-io==0.11.0','fastavro','cloudml-hypertune']\n",
    "\n",
    "setup(\n",
    "    name='trainer',\n",
    "    version='0.1',\n",
    "    install_requires=REQUIRED_PACKAGES,\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description='My training application package.'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Create the Training Module\n",
    "\n",
    "The following cell is the Driver program of the entire module. It contains the functions to generate the Train and Evaluation Datasets along with the design of the model architecture and Model Training steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model filename: evaluation1/trainer.py\n"
     ]
    }
   ],
   "source": [
    "MODEL_FILE = MODEL_FILE_PATH + \"/trainer.py\"\n",
    "print(\"model filename: \" + MODEL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing evaluation1/trainer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $MODEL_FILE\n",
    "import os\n",
    "import hypertune\n",
    "import argparse\n",
    "from io import BytesIO\n",
    "import tensorflow as tf\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.python.lib.io import file_io\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow_io.bigquery import BigQueryClient\n",
    "from tensorflow_io.bigquery import BigQueryReadSession\n",
    "\n",
    "\n",
    "#Defining Static Params\n",
    "params = \\\n",
    "    {\n",
    "        \"use_seed\": True,\n",
    "        \"num_gpus\": 0,\n",
    "        \"use_tpu\": False,\n",
    "        \"tpu\": None,\n",
    "        \"tpu_zone\": None,\n",
    "        \"tpu_gcp_project\": None,\n",
    "        \"beta1\": 0.9,\n",
    "        \"beta2\": 0.999,\n",
    "        \"epsilon\": 0.1,\n",
    "        \"use_xla_for_gpu\": False,\n",
    "        \"learning_rate\" : 0.09\n",
    "    }\n",
    "\n",
    "def get_max_min(column, restriction_rows='trip_start_timestamp <= 1546300800'):\n",
    "    \n",
    "    PROJECT_ID, DATASET_ID, TRAINING_TABLE_ID,  = 'us-gcp-ame-con-01e-npd-1.chcg_dtst.tx_trps_undrsmpld'.split('.')\n",
    "    \n",
    "    query = 'SELECT Max({}) as max, Min({}) as min FROM `{}`.{}.{} where {}'.format(column, column,PROJECT_ID, DATASET_ID, TRAINING_TABLE_ID,\n",
    "                                                                                    restriction_rows)\n",
    "    \n",
    "    client = bigquery.Client(project=PROJECT_ID)\n",
    "    \n",
    "    dataset_ref = client.dataset(DATASET_ID)\n",
    "    \n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    \n",
    "    query_job = client.query(query, job_config=job_config)\n",
    "    \n",
    "    result = query_job.to_dataframe()\n",
    "    \n",
    "    return result.values[0]\n",
    "\n",
    "def get_categorical_feature_values(column):\n",
    "    \n",
    "    PROJECT_ID, DATASET_ID, TRAINING_TABLE_ID,  = 'us-gcp-ame-con-01e-npd-1.chcg_dtst.tx_trps_undrsmpld'.split('.')\n",
    "    \n",
    "    query = 'SELECT DISTINCT {} FROM `{}`.{}.{}'.format(column, PROJECT_ID, DATASET_ID, TRAINING_TABLE_ID)\n",
    "    \n",
    "    client = bigquery.Client(project=PROJECT_ID)\n",
    "    \n",
    "    dataset_ref = client.dataset(DATASET_ID)\n",
    "    \n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    \n",
    "    query_job = client.query(query, job_config=job_config)\n",
    "    \n",
    "    result = query_job.to_dataframe()\n",
    "    \n",
    "    return result.values[:,0]\n",
    "\n",
    "def features_and_labels(features):\n",
    "    \n",
    "    label = features.pop('side')\n",
    "    \n",
    "    drop_column = features.pop('trip_start_timestamp')\n",
    "    \n",
    "    return features, label\n",
    "\n",
    "def read_dataset( row_restriction, batch_size=2084):\n",
    "    \n",
    "    client = BigQueryClient()\n",
    "    \n",
    "    GCP_PROJECT_ID='us-gcp-ame-con-01e-npd-1'  \n",
    "    \n",
    "    COL_NAMES = ['payment_type','side','trip_start_timestamp','hour','day_of_week','month','weekend','special_days',\n",
    "                 'trip_total','trip_miles','crime_rating','avg_prcp','avg_snow','avg_tavg']\n",
    "    \n",
    "    COL_TYPES = [dtypes.string, dtypes.int64, dtypes.int64] + [dtypes.int64 for i in range(0,5)] + [dtypes.float64 for i in range(0,6)]\n",
    "    \n",
    "    DATASET_GCP_PROJECT_ID, DATASET_ID, TABLE_ID,  = 'us-gcp-ame-con-01e-npd-1.chcg_dtst.tx_trps_undrsmpld'.split('.')\n",
    "    \n",
    "    bqsession = client.read_session(\n",
    "        \"projects/\" + GCP_PROJECT_ID,\n",
    "        DATASET_GCP_PROJECT_ID, TABLE_ID, DATASET_ID,\n",
    "        COL_NAMES, COL_TYPES,\n",
    "        requested_streams=2,\n",
    "        row_restriction=row_restriction)\n",
    "    \n",
    "    dataset = bqsession.parallel_read_rows(num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    return dataset.prefetch(1).map(features_and_labels).shuffle(batch_size*100).batch(batch_size)\n",
    "\n",
    "def copy_file_to_gcs(job_dir, file_path):\n",
    "    \n",
    "    with file_io.FileIO(file_path, mode='rb') as input_f:\n",
    "        with file_io.FileIO(os.path.join(job_dir, file_path), mode='w+') as fp:\n",
    "            fp.write(input_f.read())\n",
    "\n",
    "def train_and_evaluate(params):\n",
    "    \n",
    "    job_dir = params['job_dir']\n",
    "    MODEL_DIR = params['SUB_MODEL_DIRECTORY']\n",
    "    MODEL = params['MODEL']\n",
    "    train_batch_size = params['train_batch_size']\n",
    "    eval_batch_size = params['eval_batch_size']\n",
    "    train_epochs = params['num_epochs']\n",
    "    train_restriction = params['train_restriction']\n",
    "    eval_restriction = params['eval_restriction']\n",
    "    train_df = read_dataset(train_restriction, train_batch_size)\n",
    "    eval_df = read_dataset(eval_restriction, eval_batch_size)\n",
    "    \n",
    "    print(\"Data Ingested from the source\")\n",
    "\n",
    "    feature_columns = []\n",
    "\n",
    "    # numeric cols\n",
    "    def get_scal(feature):\n",
    "        \n",
    "        value = get_max_min(feature, params['train_restriction'])\n",
    "        \n",
    "        max_value, min_value = value[0], value[1]\n",
    "        \n",
    "        def minmax(x):\n",
    "            \n",
    "            mini = max_value\n",
    "            \n",
    "            maxi = min_value\n",
    "            \n",
    "            return (x - mini)/(maxi-mini)\n",
    "        \n",
    "        return(minmax)\n",
    "\n",
    "    for header in ['trip_total','trip_miles','crime_rating','avg_prcp','avg_snow','avg_tavg']:\n",
    "        \n",
    "        scal_input_fn = get_scal(header)\n",
    "        \n",
    "        feature_columns.append(feature_column.numeric_column(header, normalizer_fn = scal_input_fn ))\n",
    "\n",
    "    hpt = hypertune.HyperTune()\n",
    "\n",
    "        \n",
    "    class MyMetricCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            hpt.report_hyperparameter_tuning_metric(\n",
    "                hyperparameter_metric_tag='val_loss',\n",
    "                metric_value=logs['val_loss'],\n",
    "                global_step=epoch)\n",
    "            \n",
    "           \n",
    "    \n",
    "    tb_log = tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=os.path.join(job_dir, '{}/{}'.format(MODEL_DIR,'logs')),\n",
    "        histogram_freq=0,\n",
    "        write_graph=True,\n",
    "        update_freq='epoch',\n",
    "        embeddings_freq=0)\n",
    "\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "            lr=params[\"learning_rate\"],\n",
    "            beta_1=params[\"beta1\"],\n",
    "            beta_2=params[\"beta2\"],\n",
    "            epsilon=params[\"epsilon\"])\n",
    "\n",
    "    crossed_feature = feature_column.crossed_column(['hour','month','day_of_week'], hash_bucket_size=1000)\n",
    "    \n",
    "    crossed_feature = feature_column.indicator_column(crossed_feature)\n",
    "    \n",
    "    feature_columns.append(crossed_feature)\n",
    "    \n",
    "    # categorical cols\n",
    "    for header in ['payment_type','weekend','special_days']:\n",
    "        \n",
    "      categorical_feature = feature_column.categorical_column_with_vocabulary_list(\n",
    "            header, get_categorical_feature_values(header))\n",
    "    \n",
    "      categorical_feature_one_hot = feature_column.indicator_column(categorical_feature)\n",
    "        \n",
    "      feature_columns.append(categorical_feature_one_hot)\n",
    "\n",
    "    feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n",
    "\n",
    "    print(\"Model initialised\")\n",
    "\n",
    "    Dense = tf.keras.layers.Dense\n",
    "    \n",
    "    model = tf.keras.Sequential(\n",
    "      [\n",
    "        feature_layer,\n",
    "          Dense(45, activation=tf.nn.relu),\n",
    "          Dense(36, activation=tf.nn.relu),\n",
    "          Dense(27, activation=tf.nn.relu),\n",
    "          Dense(18, activation=tf.nn.relu),\n",
    "          Dense(9, activation=\"softmax\")\n",
    "      ])\n",
    "  \n",
    "    model.compile(\n",
    "       loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    print(\"Model training\")\n",
    "    \n",
    "    model.fit(train_df,\n",
    "              epochs=train_epochs,\n",
    "              validation_data=eval_df,\n",
    "              verbose=1,\n",
    "              callbacks=[MyMetricCallback()])\n",
    "    \n",
    "    if job_dir.startswith('gs://'):\n",
    "        \n",
    "        model.save(params['MODEL'],save_format='h5')\n",
    "        \n",
    "        copy_file_to_gcs(job_dir+'/{}'.format(MODEL_DIR), MODEL)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        model.save(os.path.join(job_dir, '{}/{}'.format(MODEL_DIR, MODEL)), save_format='h5')\n",
    "\n",
    "    tf.saved_model.save(model, os.path.join(job_dir, '{}/{}'.format(MODEL_DIR,\"export\")))\n",
    "    \n",
    "    return 1\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--job-dir',\n",
    "        type=str,\n",
    "        help='GCS or local dir to write checkpoints and export model',\n",
    "        default='/tmp/chicago-keras')\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--train-batch-size',\n",
    "        type=int,\n",
    "        default=512,\n",
    "        help='Batch size for training steps')\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--eval-batch-size',\n",
    "        type=int,\n",
    "        default=512,\n",
    "        help='Batch size for evaluation steps')\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--learning-rate',\n",
    "        type=float,\n",
    "        default=0.00714,\n",
    "        help='Learning rate for SGD')\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--beta1',\n",
    "        type=float,\n",
    "        default=0.9,\n",
    "        help='Learning rate for SGD')\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--beta2',\n",
    "        type=float,\n",
    "        default=0.99,\n",
    "        help='Learning rate for SGD')\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--epsilon',\n",
    "        type=float,\n",
    "        default=0.99,\n",
    "        help='Learning rate for SGD')\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--sub-model-directory',\n",
    "        type=str,\n",
    "        default='Experiment_One',\n",
    "        help='Sub Directory structure for the expirement')\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--train-restriction',\n",
    "        type=str,\n",
    "        default=\"'trip_start_timestamp <= 1546300800'\",\n",
    "        help='Retriction for Train Set')\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--num-epochs',\n",
    "        type=int,\n",
    "        default=114,\n",
    "        help='Maximum number of epochs on which to train')\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--eval-restriction',\n",
    "        type=str,\n",
    "        default=\"'trip_start_timestamp > 1546300800 and trip_start_timestamp<=1567276140'\",\n",
    "        help='Restriction for Evaluation Set')\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "    print(\"Args available {}\".format(args))\n",
    "    params['SUB_MODEL_DIRECTORY'] = args.sub_model_directory\n",
    "    params['MODEL'] = params['SUB_MODEL_DIRECTORY']+\".h5\"\n",
    "    params['learning_rate'] = args.learning_rate\n",
    "    params['beta1'] = args.beta1\n",
    "    params['beta2'] = args.beta2\n",
    "    params['epsilon'] = args.epsilon\n",
    "    params['num_epochs'] = args.num_epochs\n",
    "    params['train_batch_size'] = args.train_batch_size\n",
    "    params['job_dir'] = args.job_dir\n",
    "    params['train_restriction'] = args.train_restriction\n",
    "    params['eval_restriction'] = args.eval_restriction\n",
    "    params['eval_batch_size'] = args.eval_batch_size\n",
    "    train_and_evaluate(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. HyperParameters Configuration File\n",
    "The Hyperparameters can be defined in a Configuration File and be used to run experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration File : config.yaml\n"
     ]
    }
   ],
   "source": [
    "\n",
    "CONFIG_FILE = 'config.yaml'\n",
    "\n",
    "print(\"Configuration File : \"+ CONFIG_FILE )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile $CONFIG_FILE\n",
    "trainingInput:\n",
    "  hyperparameters:\n",
    "    goal: MINIMIZE\n",
    "    hyperparameterMetricTag: val_loss\n",
    "    enableTrialEarlyStopping: True\n",
    "    maxTrials: 48\n",
    "    maxParallelTrials: 4\n",
    "    params:\n",
    "      - parameterName: learning_rate\n",
    "        type: DOUBLE\n",
    "        minValue: 0.0001\n",
    "        maxValue: 0.01\n",
    "        scaleType: UNIT_REVERSE_LOG_SCALE\n",
    "      - parameterName: beta1\n",
    "        type: DOUBLE\n",
    "        minValue: 0.0009\n",
    "        maxValue: 0.9\n",
    "        scaleType: UNIT_REVERSE_LOG_SCALE\n",
    "      - parameterName: beta2\n",
    "        type: DOUBLE\n",
    "        minValue: 0.0009\n",
    "        maxValue: 0.9\n",
    "        scaleType: UNIT_REVERSE_LOG_SCALE\n",
    "      - parameterName: epsilon\n",
    "        type: DOUBLE\n",
    "        minValue: 0.0001\n",
    "        maxValue: 0.01\n",
    "        scaleType: UNIT_REVERSE_LOG_SCALE\n",
    "      - parameterName: train_batch_size\n",
    "        type: INTEGER\n",
    "        minValue: 1000\n",
    "        maxValue: 20000\n",
    "        scaleType: UNIT_LOG_SCALE\n",
    "      - parameterName: num-epochs\n",
    "        type: INTEGER\n",
    "        minValue: 512\n",
    "        maxValue: 1000\n",
    "        scaleType: UNIT_LINEAR_SCALE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Create placeholders for Python Package Structure\n",
    "To convert a Python file into a package a set of arbritary files are needed. This step creates those files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INIT file: evaluation1/__init__.py\n"
     ]
    }
   ],
   "source": [
    "INIT_FILE = MODEL_FILE_PATH + \"/__init__.py\"\n",
    "print(\"INIT file: \" + INIT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing evaluation1/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $INIT_FILE\n",
    "# __init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'evaluation1.trainer'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## setting the 'module-name' variable dynamically here.\n",
    "trainer_module = PACKAGE_NAME + '.trainer'\n",
    "trainer_module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Platform Job Trigger with HyperParameter Tuning\n",
    "\n",
    "This triggers the AI Platform Job and transfers the logs to the Kernel. You can also check the logs in the AI Platform jobs section. Further more, to make sure that the jobs are unique, a time stamp is appended to the Job Name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [Evaluation_One_Final_v320210226_235328] submitted successfully.\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform jobs submit training $JOB_NAME$(date +\"%Y%m%d_%H%M%S\") \\\n",
    "  --package-path $MODEL_FILE_PATH/ \\\n",
    "  --module-name $trainer_module \\\n",
    "  --region $REGION \\\n",
    "  --job-dir $JOB_DIR \\\n",
    "  --python-version 3.7 \\\n",
    "  --stream-logs \\\n",
    "  --runtime-version 2.2 \\\n",
    "  --config $CONFIG_FILE \\\n",
    "  -- \\\n",
    "  --sub-model-directory $MODEL_NAME \\\n",
    "  --train-restriction $TRAIN_RESTRICTION \\\n",
    "  --eval-restriction $EVAL_RESTRICTION \n",
    "\n",
    "\n",
    "#Uncomment the steps below to enable distributed training\n",
    "\n",
    "#   --scale-tier custom \\\n",
    "#   --master-machine-type n1-highmem-8 \\\n",
    "#   --worker-machine-type n1-highmem-4 \\\n",
    "#   --parameter-server-machine-type n1-highmem-4 \\\n",
    "#   --parameter-server-count 1 \\\n",
    "#   --worker-count 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the model architecture remains the same for all the models, we are only going to run one Hyper Parameter Job and use the final parameters for all the Experiments"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Tce3stUlHN0L"
   ],
   "name": "bigquery.ipynb",
   "toc_visible": true
  },
  "environment": {
   "name": "tf2-gpu.2-1.m61",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m61"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
