{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tce3stUlHN0L"
   },
   "source": [
    "##### Copyright Deloitte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qFdPvlXBOdUN"
   },
   "source": [
    "# Chicago Taxi Geographic zones prediction (Training) Model Experiment - 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xHxb-dlhMIzW"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook is intended to showcase the steps taken to train a model for the Chicago Taxi Geographic zones prediction. Before we jump into the detailed steps used, let us get an eagle view of the entire process. Model Development has the following life cycle:\n",
    "\n",
    "1. Data Preprocessing\n",
    "2. Model Design and Training **[Here]**\n",
    "3. Model Evaluation and HyperParameter Tuning\n",
    "4. Model Deployment and Inference\n",
    "\n",
    "Our current hypothesis dictates the prediction of the High Demand Geographic zones based on several factors (more details provided in the technical document and white paper). This notebook details primarily with the Model Training aspect of the Modeling Life Cycle and showcases the necessary steps required to run a training job using AI Platform.\n",
    "\n",
    "\n",
    "*Note: This Experiment was run on Chicago Taxi Data includingdata for the year 2020*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MUXex9ctTuDB"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4YsfgDMZW5g6"
   },
   "source": [
    "Set up your GCP project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "#### 1. Define your configuration Variables :\n",
    "\n",
    "This is the process of providing static configuration details such as the Project Name, Bucket Name etc along with some variable details such as the Number of Epochs etc. The static details can be changed to work with any other GCP project while the Variable details can be used to run HyperParameter Tuning jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "# Use project ID from console page\n",
    "!gcloud config set project us-gcp-ame-con-01e-npd-1\n",
    "\n",
    "#project name\n",
    "PROJECT_NAME = \"us-gcp-ame-con-01e-npd-1\"\n",
    "\n",
    "# Shows up in models pane of AI platform, helps you track your key project\n",
    "MODEL_NAME = \"Experiment_Three\"\n",
    "\n",
    "# This defines your each experiment, try to keep it meaningful\n",
    "MODEL_VERSION = \"v3\"\n",
    "\n",
    "#region\n",
    "REGION = \"us-east4\"\n",
    "\n",
    "# JOB_NAME is auto-incremental. It will be appended by the latest timestamp when submitting the training job.\n",
    "JOB_NAME = MODEL_NAME + '_' + MODEL_VERSION\n",
    "\n",
    "#Google Storage Bucket\n",
    "BUCKET_NAME = \"us-gcp-ame-con-01e-npd-1-modelartifacts\"\n",
    "\n",
    "#sub folder name \n",
    "SUB_FOLDER_NAME = MODEL_NAME + \"/\" + MODEL_VERSION\n",
    "\n",
    "# Creates gs directory with this name - persists transient files during AI Platform jobs tun\n",
    "JOB_DIR = 'gs://' + BUCKET_NAME + '/' + SUB_FOLDER_NAME\n",
    "\n",
    "# Should be same as JOB_DIR with model_save as folder added\n",
    "MODEL_DIR = JOB_DIR + \"/model_save/\"\n",
    "\n",
    "#The number of times the data is shown to the Model. This is a hyper parameter and can be tuned\n",
    "NUM_EPOCHS = 114\n",
    "\n",
    "#The restriction placed on the dataset to retrive our train data. \n",
    "TRAIN_RESTRICTION = \"'trip_start_timestamp <= 1546300800'\"\n",
    "\n",
    "#The restriction placed on the dataset to retrieve our validation data\n",
    "EVAL_RESTRICTION = \"'trip_start_timestamp > 1546300800 and trip_start_timestamp<=1567276140'\"\n",
    "\n",
    "#Batch size for training\n",
    "TRAIN_BATCH_SIZE = 512\n",
    "\n",
    "#Batch size for evaluation\n",
    "EVAL_BATCH_SIZE = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Create a skeleton for the Trainer\n",
    "According to the AI Platform documentation the training code has to be modularised into a Trainer package with a defined layout. The following steps create that layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Delete the folder if it already exist or change the MODEL_VERSION to keep it unique\n",
    "import os, shutil\n",
    "\n",
    "PACKAGE_NAME = 'experiment3'\n",
    "\n",
    "MODEL_FILE_PATH = '{}'.format(PACKAGE_NAME)\n",
    "\n",
    "if os.path.isdir(MODEL_FILE_PATH):\n",
    "    \n",
    "    shutil.rmtree(MODEL_FILE_PATH)\n",
    "    \n",
    "!mkdir $MODEL_FILE_PATH\n",
    "\n",
    "trainer_path = MODEL_FILE_PATH + '/trainer.py'\n",
    "\n",
    "init_path = MODEL_FILE_PATH + '/__init__.py'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Create the Setup File\n",
    "\n",
    "The Setup File is required to download any additional package that was used during the Modeling process. This is placed outside the Trainer model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setup filename: setup.py\n"
     ]
    }
   ],
   "source": [
    "SETUP_FILENAME =  \"setup.py\"\n",
    "\n",
    "print(\"setup filename: \" + SETUP_FILENAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $SETUP_FILENAME\n",
    "\n",
    "# Setup file to install necessary packages\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "# You can define all the packages you need for your job below along with the versions\n",
    "\n",
    "REQUIRED_PACKAGES = ['tensorflow-io==0.11.0','fastavro']\n",
    "\n",
    "setup(\n",
    "    name='trainer',\n",
    "    version='0.1',\n",
    "    install_requires=REQUIRED_PACKAGES,\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description='My training application package.'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Create the Training Module\n",
    "\n",
    "The following cell is the Driver program of the entire module. It contains the functions to generate the Train and Evaluation Datasets along with the design of the model architecture and Model Training steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model filename: experiment3/trainer.py\n"
     ]
    }
   ],
   "source": [
    "MODEL_FILE = MODEL_FILE_PATH + \"/trainer.py\"\n",
    "print(\"model filename: \" + MODEL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing experiment3/trainer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $MODEL_FILE\n",
    "import os\n",
    "import argparse\n",
    "from io import BytesIO\n",
    "import tensorflow as tf\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.python.lib.io import file_io\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow_io.bigquery import BigQueryClient\n",
    "from tensorflow_io.bigquery import BigQueryReadSession\n",
    "\n",
    "\n",
    "#Defining Static Params\n",
    "params = \\\n",
    "    {\n",
    "        \"use_seed\": True,\n",
    "        \"num_gpus\": 0,\n",
    "        \"use_tpu\": False,\n",
    "        \"tpu\": None,\n",
    "        \"tpu_zone\": None,\n",
    "        \"tpu_gcp_project\": None,\n",
    "        \"beta1\": 0.9,\n",
    "        \"beta2\": 0.999,\n",
    "        \"epsilon\": 0.1,\n",
    "        \"use_xla_for_gpu\": False,\n",
    "        \"learning_rate\" : 0.00714\n",
    "    }\n",
    "\n",
    "def get_max_min(column, restriction_rows='trip_start_timestamp <= 1546300800'):\n",
    "    \n",
    "    PROJECT_ID, DATASET_ID, TRAINING_TABLE_ID,  = 'us-gcp-ame-con-01e-npd-1.chcg_dtst.tx_trps_undrsmpld'.split('.')\n",
    "    \n",
    "    query = 'SELECT Max({}) as max, Min({}) as min FROM `{}`.{}.{} where {}'.format(column, column,PROJECT_ID, DATASET_ID, TRAINING_TABLE_ID,\n",
    "                                                                                    restriction_rows)\n",
    "    \n",
    "    client = bigquery.Client(project=PROJECT_ID)\n",
    "    \n",
    "    dataset_ref = client.dataset(DATASET_ID)\n",
    "    \n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    \n",
    "    query_job = client.query(query, job_config=job_config)\n",
    "    \n",
    "    result = query_job.to_dataframe()\n",
    "    \n",
    "    return result.values[0]\n",
    "\n",
    "def get_categorical_feature_values(column):\n",
    "    \n",
    "    PROJECT_ID, DATASET_ID, TRAINING_TABLE_ID,  = 'us-gcp-ame-con-01e-npd-1.chcg_dtst.tx_trps_undrsmpld'.split('.')\n",
    "    \n",
    "    query = 'SELECT DISTINCT {} FROM `{}`.{}.{}'.format(column, PROJECT_ID, DATASET_ID, TRAINING_TABLE_ID)\n",
    "    \n",
    "    client = bigquery.Client(project=PROJECT_ID)\n",
    "    \n",
    "    dataset_ref = client.dataset(DATASET_ID)\n",
    "    \n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    \n",
    "    query_job = client.query(query, job_config=job_config)\n",
    "    \n",
    "    result = query_job.to_dataframe()\n",
    "    \n",
    "    return result.values[:,0]\n",
    "\n",
    "def features_and_labels(features):\n",
    "    \n",
    "    label = features.pop('side')\n",
    "    \n",
    "    drop_columns = ['trip_start_timestamp','payment_type','trip_miles','trip_total' ]\n",
    "    \n",
    "    for column in drop_columns:\n",
    "        dropped_column = features.pop(column)\n",
    "    \n",
    "    return features, label\n",
    "\n",
    "def read_dataset( row_restriction, batch_size=2084):\n",
    "    \n",
    "    client = BigQueryClient()\n",
    "    \n",
    "    GCP_PROJECT_ID='us-gcp-ame-con-01e-npd-1'  \n",
    "    \n",
    "    COL_NAMES = ['payment_type','side','trip_start_timestamp','hour','day_of_week','month','weekend','special_days',\n",
    "                 'trip_total','trip_miles','crime_rating','avg_prcp','avg_snow','avg_tavg']\n",
    "    \n",
    "    COL_TYPES = [dtypes.string, dtypes.int64, dtypes.int64] + [dtypes.int64 for i in range(0,5)] + [dtypes.float64 for i in range(0,6)]\n",
    "    \n",
    "    DATASET_GCP_PROJECT_ID, DATASET_ID, TABLE_ID,  = 'us-gcp-ame-con-01e-npd-1.chcg_dtst.tx_trps_undrsmpld'.split('.')\n",
    "    \n",
    "    bqsession = client.read_session(\n",
    "        \"projects/\" + GCP_PROJECT_ID,\n",
    "        DATASET_GCP_PROJECT_ID, TABLE_ID, DATASET_ID,\n",
    "        COL_NAMES, COL_TYPES,\n",
    "        requested_streams=2,\n",
    "        row_restriction=row_restriction)\n",
    "    \n",
    "    dataset = bqsession.parallel_read_rows(num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    return dataset.prefetch(1).map(features_and_labels).shuffle(batch_size*100).batch(batch_size)\n",
    "\n",
    "def copy_file_to_gcs(job_dir, file_path):\n",
    "    \n",
    "    with file_io.FileIO(file_path, mode='rb') as input_f:\n",
    "        with file_io.FileIO(os.path.join(job_dir, file_path), mode='w+') as fp:\n",
    "            fp.write(input_f.read())\n",
    "\n",
    "def train_and_evaluate(params):\n",
    "    \n",
    "    job_dir = params['job_dir']\n",
    "    MODEL_DIR = params['SUB_MODEL_DIRECTORY']\n",
    "    MODEL = params['MODEL']\n",
    "    train_batch_size = params['train_batch_size']\n",
    "    eval_batch_size = params['eval_batch_size']\n",
    "    train_epochs = params['train_epochs']\n",
    "    train_restriction = params['train_restriction']\n",
    "    eval_restriction = params['eval_restriction']\n",
    "    train_df = read_dataset(train_restriction, train_batch_size)\n",
    "    eval_df = read_dataset(eval_restriction, eval_batch_size)\n",
    "    \n",
    "    print(\"Data Ingested from the source\")\n",
    "\n",
    "    feature_columns = []\n",
    "\n",
    "    # numeric cols\n",
    "    def get_scal(feature):\n",
    "        \n",
    "        value = get_max_min(feature, train_restriction)\n",
    "        \n",
    "        max_value, min_value = value[0], value[1]\n",
    "        \n",
    "        def minmax(x):\n",
    "            \n",
    "            mini = max_value\n",
    "            \n",
    "            maxi = min_value\n",
    "            \n",
    "            return (x - mini)/(maxi-mini)\n",
    "        \n",
    "        return(minmax)\n",
    "\n",
    "    for header in ['crime_rating','avg_prcp','avg_snow','avg_tavg']:\n",
    "        \n",
    "        scal_input_fn = get_scal(header)\n",
    "        \n",
    "        feature_columns.append(feature_column.numeric_column(header, normalizer_fn = scal_input_fn ))\n",
    "\n",
    "    callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        MODEL, \n",
    "        monitor='accuracy', save_best_only=True, \n",
    "        verbose=0, mode='max', period=1)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "            lr=params[\"learning_rate\"],\n",
    "            beta_1=params[\"beta1\"],\n",
    "            beta_2=params[\"beta2\"],\n",
    "            epsilon=params[\"epsilon\"])\n",
    "\n",
    "    crossed_feature = feature_column.crossed_column(['hour','month','day_of_week'], hash_bucket_size=1000)\n",
    "    \n",
    "    crossed_feature = feature_column.indicator_column(crossed_feature)\n",
    "    \n",
    "    feature_columns.append(crossed_feature)\n",
    "    \n",
    "    # categorical cols\n",
    "    for header in ['weekend','special_days']:\n",
    "        \n",
    "      categorical_feature = feature_column.categorical_column_with_vocabulary_list(\n",
    "            header, get_categorical_feature_values(header))\n",
    "    \n",
    "      categorical_feature_one_hot = feature_column.indicator_column(categorical_feature)\n",
    "        \n",
    "      feature_columns.append(categorical_feature_one_hot)\n",
    "\n",
    "    feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n",
    "\n",
    "    print(\"Model initialised\")\n",
    "\n",
    "    Dense = tf.keras.layers.Dense\n",
    "\n",
    "    model = tf.keras.Sequential(\n",
    "      [\n",
    "        feature_layer,\n",
    "          Dense(45, activation=tf.nn.relu),\n",
    "          Dense(36, activation=tf.nn.relu),\n",
    "          Dense(27, activation=tf.nn.relu),\n",
    "          Dense(18, activation=tf.nn.relu),\n",
    "          Dense(9, activation=\"softmax\")\n",
    "      ])\n",
    "  \n",
    "    model.compile(\n",
    "       loss='sparse_categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
    "\n",
    "    print(\"Model training\")\n",
    "    \n",
    "    model.fit(train_df,\n",
    "              epochs=train_epochs,\n",
    "              validation_data=eval_df,\n",
    "              verbose=1)\n",
    "    \n",
    "    if job_dir.startswith('gs://'):\n",
    "        \n",
    "        model.save(params['MODEL'],save_format='h5')\n",
    "        \n",
    "        copy_file_to_gcs(job_dir+'/{}'.format(MODEL_DIR), MODEL)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        model.save(os.path.join(job_dir, '{}/{}'.format(MODEL_DIR, MODEL)), save_format='h5')\n",
    "\n",
    "    tf.saved_model.save(model, os.path.join(job_dir, '{}/{}'.format(MODEL_DIR,\"export\")))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--job-dir',\n",
    "        type=str,\n",
    "        help='GCS or local dir to write checkpoints and export model',\n",
    "        default='/tmp/chicago-keras')\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--train-batch-size',\n",
    "        type=int,\n",
    "        default=10000,\n",
    "        help='Batch size for training steps')\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--eval-batch-size',\n",
    "        type=int,\n",
    "        default=1000,\n",
    "        help='Batch size for evaluation steps')\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--sub-model-directory',\n",
    "        type=str,\n",
    "        default='Experiment_One',\n",
    "        help='Sub Directory structure for the expirement')\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--train-restriction',\n",
    "        type=str,\n",
    "        default=\"'trip_start_timestamp <= 1546300800'\",\n",
    "        help='Retriction for Train Set')\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--num-epochs',\n",
    "        type=int,\n",
    "        default=100,\n",
    "        help='Maximum number of epochs on which to train')\n",
    "    \n",
    "    parser.add_argument(\n",
    "        '--eval-restriction',\n",
    "        type=str,\n",
    "        default=\"'trip_start_timestamp > 1546300800 and trip_start_timestamp<=1567276140'\",\n",
    "        help='Restriction for Evaluation Set')\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    params['SUB_MODEL_DIRECTORY'] = args.sub_model_directory\n",
    "    params['MODEL'] = params['SUB_MODEL_DIRECTORY']+\".h5\"\n",
    "    params['train_epochs'] = args.num_epochs\n",
    "    params['train_batch_size'] = args.train_batch_size\n",
    "    params['job_dir'] = args.job_dir\n",
    "    params['train_restriction'] = args.train_restriction\n",
    "    params['eval_restriction'] = args.eval_restriction\n",
    "    params['eval_batch_size'] = args.eval_batch_size\n",
    "    train_and_evaluate(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Create placeholders for Python Package Structure\n",
    "To convert a Python file into a package a set of arbritary files are needed. This step creates those files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INIT file: experiment3/__init__.py\n"
     ]
    }
   ],
   "source": [
    "INIT_FILE = MODEL_FILE_PATH + \"/__init__.py\"\n",
    "print(\"INIT file: \" + INIT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing experiment3/__init__.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $INIT_FILE\n",
    "# __init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'experiment3.trainer'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## setting the 'module-name' variable dynamically here.\n",
    "trainer_module = PACKAGE_NAME + '.trainer'\n",
    "trainer_module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Platform Job Trigger\n",
    "\n",
    "This triggers the AI Platform Job and transfers the logs to the Kernel. You can also check the logs in the AI Platform jobs section. Further more, to make sure that the jobs are unique, a time stamp is appended to the Job Name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [Experiment_Three_v3] submitted successfully.\n",
      "INFO\t2021-02-26 23:45:32 +0000\tservice\t\tValidating job requirements...\n",
      "INFO\t2021-02-26 23:45:32 +0000\tservice\t\tJob creation request has been successfully validated.\n",
      "INFO\t2021-02-26 23:45:33 +0000\tservice\t\tJob Experiment_Three_v3 is queued.\n",
      "INFO\t2021-02-26 23:45:45 +0000\tservice\t\tThis job is number 2 in the queue and requires 4.0 N1/E2 CPUs, 0 accelerators, 0Gb standard disks and 100Gb ssd disks. The project is using 16.0 N1/E2 CPUs out of 22.0 C2, 22.0 N2, 2200.0 N1/E2, 9000.0 preemptible allowed, 0 accelerators out of 0 A100, 0 TPU_V2_POD, 0 TPU_V3_POD, 14 T4, 16 TPU_V2, 16 TPU_V3, 55 K80, 55 P100, 8 P4, 8 V100 allowed, 0Gb standard disks out of 180000 allowed and 1200Gb ssd disks out of 75000 allowed across all regions.The project is using 16.0 N1/E2 CPUs out of 20.0 N1/E2, 22.0 C2, 22.0 N2, 9000.0 preemptible allowed, 0 accelerators out of 0 A100, 0 K80, 0 P100, 0 T4, 0 TPU_V2, 0 TPU_V2_POD, 0 TPU_V3, 0 TPU_V3_POD, 0 V100, 1 P4 allowed, 0Gb standard disks out of 4000 allowed and 1200Gb ssd disks out of 500 allowed in the region us-east4.\n",
      "INFO\t2021-02-26 23:50:16 +0000\tservice\t\tWaiting for job to be provisioned.\n",
      "INFO\t2021-02-26 23:50:18 +0000\tservice\t\tWaiting for training program to start.\n"
     ]
    }
   ],
   "source": [
    "!gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "  --package-path $MODEL_FILE_PATH/ \\\n",
    "  --module-name $trainer_module \\\n",
    "  --region $REGION \\\n",
    "  --job-dir $JOB_DIR \\\n",
    "  --python-version 3.7 \\\n",
    "  --stream-logs \\\n",
    "  --runtime-version 2.2 \\\n",
    "  -- \\\n",
    "  --sub-model-directory $MODEL_NAME \\\n",
    "  --num-epochs $NUM_EPOCHS \\\n",
    "  --train-restriction $TRAIN_RESTRICTION \\\n",
    "  --eval-restriction $EVAL_RESTRICTION \\\n",
    "  --train-batch-size $TRAIN_BATCH_SIZE \\\n",
    "  --eval-batch-size $EVAL_BATCH_SIZE\n",
    "\n",
    "\n",
    "#Uncomment the steps below to enable distributed training\n",
    "\n",
    "#   --scale-tier custom \\\n",
    "#   --master-machine-type n1-highmem-8 \\\n",
    "#   --worker-machine-type n1-highmem-4 \\\n",
    "#   --parameter-server-machine-type n1-highmem-4 \\\n",
    "#   --parameter-server-count 1 \\\n",
    "#   --worker-count 1\n",
    "#   $(date +\"%Y%m%d_%H%M%S\") Add this to make the Job Unique\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Tce3stUlHN0L"
   ],
   "name": "bigquery.ipynb",
   "toc_visible": true
  },
  "environment": {
   "name": "tf2-gpu.2-1.m61",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m61"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
